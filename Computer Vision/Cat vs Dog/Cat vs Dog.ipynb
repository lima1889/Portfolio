{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae3733a7",
   "metadata": {},
   "source": [
    "### Import the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "60217f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image, image_dataset_from_directory\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications.mobilenet import MobileNet, preprocess_input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c973b91",
   "metadata": {},
   "source": [
    "### Define the data features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0b4ab726",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = 'data/train'\n",
    "validation_data_dir = 'data/val'\n",
    "train_samples = 2000\n",
    "validation_samples = 1000\n",
    "num_classes = 2\n",
    "img_width, img_height = 224, 224\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6971d8d3",
   "metadata": {},
   "source": [
    "### Apply rotations, shifts and zooms on the images for data augmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ad817593",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = image.ImageDataGenerator(preprocessing_function=preprocess_input\n",
    "                                  , rotation_range=20\n",
    "                                  , width_shift_range=0.2\n",
    "                                  , height_shift_range=0.2\n",
    "                                  , zoom_range=0.2)\n",
    "val_datagen = image.ImageDataGenerator(preprocessing_function=preprocess_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c78c0b8",
   "metadata": {},
   "source": [
    "### Load the data and create the generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8faff1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "                        train_data_dir\n",
    "                        , target_size=(img_width, img_height)\n",
    "                        , batch_size=batch_size\n",
    "                        , shuffle=True\n",
    "                        , seed=12345\n",
    "                        , class_mode='categorical')\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "                        validation_data_dir\n",
    "                        , target_size=(img_width, img_height)\n",
    "                        , batch_size=batch_size\n",
    "                        , shuffle=True\n",
    "                        , seed=12345\n",
    "                        , class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d326ad0",
   "metadata": {},
   "source": [
    "### Create a function to fit the model to our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "be84ab6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_maker():\n",
    "    base_model = MobileNet(include_top=False, input_shape=(img_width, img_height, 3)) #Use the MobileNet pre-trained model \n",
    "    for layer in base_model.layers[:]:\n",
    "        layer.trainable = False #Freeze the layers\n",
    "    input = Input(shape=(img_width, img_height, 3))\n",
    "    custom_model = base_model(input) #Base model we'll use for transfer\n",
    "    custom_model = GlobalAveragePooling2D() (custom_model) #Average features per categories\n",
    "    custom_model = Dense(64, activation='relu') (custom_model) #NN layer\n",
    "    custom_model = Dropout(0.5) (custom_model) #Layer to prevent overfitting\n",
    "    predictions = Dense(num_classes, activation='softmax') (custom_model) #Final model predictions\n",
    "    return Model(inputs=input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c15b9a1",
   "metadata": {},
   "source": [
    "### Apply the function to the data and fit the model to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6d227634",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41260/4156942998.py:7: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(train_generator,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.3339 - acc: 0.8550WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 32 batches). You may need to use the repeat() function when building your dataset.\n",
      "32/32 [==============================] - 84s 3s/step - loss: 0.3339 - acc: 0.8550 - val_loss: 0.0772 - val_acc: 0.9700\n",
      "Epoch 2/20\n",
      "32/32 [==============================] - 54s 2s/step - loss: 0.1142 - acc: 0.9540\n",
      "Epoch 3/20\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.1016 - acc: 0.9620\n",
      "Epoch 4/20\n",
      "32/32 [==============================] - 54s 2s/step - loss: 0.0767 - acc: 0.9705\n",
      "Epoch 5/20\n",
      "32/32 [==============================] - 54s 2s/step - loss: 0.0744 - acc: 0.9725\n",
      "Epoch 6/20\n",
      "32/32 [==============================] - 58s 2s/step - loss: 0.0681 - acc: 0.9710\n",
      "Epoch 7/20\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.0648 - acc: 0.9795\n",
      "Epoch 8/20\n",
      "32/32 [==============================] - 54s 2s/step - loss: 0.0561 - acc: 0.9770\n",
      "Epoch 9/20\n",
      "32/32 [==============================] - 56s 2s/step - loss: 0.0721 - acc: 0.9720\n",
      "Epoch 10/20\n",
      "32/32 [==============================] - 54s 2s/step - loss: 0.0636 - acc: 0.9780\n",
      "Epoch 11/20\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.0572 - acc: 0.9825\n",
      "Epoch 12/20\n",
      "32/32 [==============================] - 55s 2s/step - loss: 0.0468 - acc: 0.9830\n",
      "Epoch 13/20\n",
      "32/32 [==============================] - 54s 2s/step - loss: 0.0521 - acc: 0.9810\n",
      "Epoch 14/20\n",
      "32/32 [==============================] - 54s 2s/step - loss: 0.0499 - acc: 0.9805\n",
      "Epoch 15/20\n",
      "32/32 [==============================] - 53s 2s/step - loss: 0.0424 - acc: 0.9855\n",
      "Epoch 16/20\n",
      "32/32 [==============================] - 54s 2s/step - loss: 0.0504 - acc: 0.9830\n",
      "Epoch 17/20\n",
      "32/32 [==============================] - 52s 2s/step - loss: 0.0389 - acc: 0.9875\n",
      "Epoch 18/20\n",
      "32/32 [==============================] - 52s 2s/step - loss: 0.0398 - acc: 0.9850\n",
      "Epoch 19/20\n",
      "32/32 [==============================] - 52s 2s/step - loss: 0.0566 - acc: 0.9790\n",
      "Epoch 20/20\n",
      "32/32 [==============================] - 51s 2s/step - loss: 0.0375 - acc: 0.9880\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6dfacd4190>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model_maker()\n",
    "model.compile(loss = 'categorical_crossentropy'\n",
    "              , optimizer = Adam(learning_rate=0.001)\n",
    "              , metrics = ['acc'])\n",
    "\n",
    "n_steps = math.ceil(float(train_samples)/batch_size)\n",
    "model.fit_generator(train_generator\n",
    "                    , steps_per_epoch = n_steps\n",
    "                    , epochs = 20\n",
    "                    , validation_data = validation_generator\n",
    "                    , validation_steps = n_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8e6682",
   "metadata": {},
   "source": [
    "Here we see that the at the end of training the model has achieved a 98.8% accuracy on the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5e75f7",
   "metadata": {},
   "source": [
    "### Save the model for easier use in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "4309f844",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c643c856",
   "metadata": {},
   "source": [
    "### Load the model and test on some images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a39ecabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "0fb011eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.0203511e-05 9.9996984e-01]\n",
      "dog\n"
     ]
    }
   ],
   "source": [
    "img = image.load_img('data/dog.193.jpg', target_size=(224,224))\n",
    "img_array = image.img_to_array(img)\n",
    "expanded_img_array = np.expand_dims(img_array, axis=0)\n",
    "preprocessed_img = preprocess_input(expanded_img_array)\n",
    "prediction = model.predict(preprocessed_img)\n",
    "print(prediction[0])\n",
    "if np.argmax(prediction[0]) == 0:\n",
    "    print('cat')\n",
    "else:\n",
    "    print('dog')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befbb53c",
   "metadata": {},
   "source": [
    "The two images below not labeled as cat are photos of my own cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "2e05c741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8296348 0.1703652]\n",
      "cat\n"
     ]
    }
   ],
   "source": [
    "img = image.load_img('chell.jpeg', target_size=(224,224))\n",
    "img_array = image.img_to_array(img)\n",
    "expanded_img_array = np.expand_dims(img_array, axis=0)\n",
    "preprocessed_img = preprocess_input(expanded_img_array)\n",
    "prediction = model.predict(preprocessed_img)\n",
    "print(prediction[0])\n",
    "if np.argmax(prediction[0]) == 0:\n",
    "    print('cat')\n",
    "else:\n",
    "    print('dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d67a2b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.9999976e-01 2.7523157e-07]\n",
      "cat\n"
     ]
    }
   ],
   "source": [
    "img = image.load_img('chell2.jpeg', target_size=(224,224))\n",
    "img_array = image.img_to_array(img)\n",
    "expanded_img_array = np.expand_dims(img_array, axis=0)\n",
    "preprocessed_img = preprocess_input(expanded_img_array)\n",
    "prediction = model.predict(preprocessed_img)\n",
    "print(prediction[0])\n",
    "if np.argmax(prediction[0]) == 0:\n",
    "    print('cat')\n",
    "else:\n",
    "    print('dog')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59733f2a",
   "metadata": {},
   "source": [
    "### Test the model with a few more images to get an estimated accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b8181c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = image.ImageDataGenerator(preprocessing_function=preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "607818dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator = test_datagen.flow_from_directory(\n",
    "                         'data/test'\n",
    "                        , target_size = (224, 224)\n",
    "                        , batch_size = 64\n",
    "                        , shuffle = True\n",
    "                        , seed = 12345\n",
    "                        , class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "93b14c28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 21s 1s/step - loss: 0.0567 - acc: 0.9830\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.05665692314505577, 0.9829999804496765]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1939593c",
   "metadata": {},
   "source": [
    "Evaluating the model on 1000 test images we see that it has a 98.3% accuracy on unseen data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
